{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The Capstone project focuses on creating a Data Warehouse for retaining the I-94 immigration records to provide different insights on the demographics of visitors entering the United States.\n",
    "The project implements a dimensional Data Model for the Immigration, Cities and Airport codes data sets and also builds out an ETL pipeline to load the data into AWS Redshift.\n",
    "A star schema is created with dimensional modeling with a fact table for the immigration data by airports/US cities.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, desc\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "Scope - The project focuses on :\n",
    "  (i)  Immigration data only for US. \n",
    "  (ii) Immigration data for a span of 4 years (2014-2018)\n",
    "\n",
    "Data Sources\n",
    "------------\n",
    "1. immigration_data_sample.csv - This data set contains immigration information for visitors like - i94 ID, arrival_Date, departure_date,\n",
    "visa_type, gender, birth_year, admsn_no, flight_no, airline\n",
    "\n",
    "2. us-cities-demographics.csv - This data set contains the demographic information for US cities like - race, gender, ethnicity, total population, total number of males/females in the city etc \n",
    "\n",
    "3. airport-codes.csv - This data set contains the airport codes, airport name, type of airport,  region and country to which the airport belongs.\n",
    "\n",
    "\n",
    "Data Model \n",
    "----------\n",
    "\n",
    "Dimensions :\n",
    "-----------\n",
    "1. dim_airport  - Dimension table for airport codes ( PK - airport_code)\n",
    "\n",
    "Columns\n",
    "------\n",
    "ident\n",
    "name\n",
    "country\n",
    "region\n",
    "co-ordinates\n",
    "\n",
    "2. dim_city_demog - Dimension table for city demographics ( PK = city name) \n",
    "\n",
    "Columns\n",
    "------\n",
    "city\n",
    "state\n",
    "state_cd\n",
    "median_Age\n",
    "male_population\n",
    "female_population\n",
    "Race\n",
    "\n",
    "3. dim_visitors - Dimension table with immigration data for US visitors ( PK = I94 admission num)\n",
    "\n",
    "Columns\n",
    "------\n",
    "admn_num\n",
    "visa_type\n",
    "transport_mode\n",
    "arr_Date\n",
    "dep_date\n",
    "birth_yr\n",
    "gender\n",
    "port_of_entry\n",
    "age (calculated field)\n",
    "\n",
    "\n",
    "Fact\n",
    "----\n",
    "4. fact_immigrations - Fact table which records the US visitors coming in by airport/cities.\n",
    "\n",
    "The ETL Data Pipeline is built using Python commands and the Data Warehouse is built in AWS.\n",
    "The data sources are staged into AWS S3 buckets, cleansed and loaded into Redshift database.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "1. immigration_data_sample.csv - This data comes from the US National Tourism and Trade Office. The dataset contains international visitor arrival statistics by world regions and select countries, type of visa, mode of transportation, age groups, states visited (first intended address only), and the top ports of entry for select countries \n",
    "\n",
    "2. us-cities-demographics.csv - This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. This data comes from the US Census Bureau's 2015 American Community Survey\n",
    "\n",
    "3. airport-codes.csv - This is a simple table of airport codes and corresponding cities. The airport codes may refer to either IATA airport code, a three-letter code which is used in passenger reservation, ticketing and baggage-handling systems, or the ICAO airport code which is a four letter code used by ATC systems and for airports that do not have an IATA airport code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "\n",
    "# read immigration data file\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "#print(df.count())\n",
    "\n",
    "#read us-cities dataset\n",
    "fname = '/home/workspace/us-cities-demographics.csv'\n",
    "df_city = pd.read_csv(fname, delimiter = \";\")\n",
    "\n",
    "#read airport-codes dataset\n",
    "fname = '/home/workspace/airport-codes_csv.csv'\n",
    "df_airport = pd.read_csv(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_city.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['KEY']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['SECRET']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#write to parquet\n",
    "#df_spark.write.parquet(\"sas_data1\")\n",
    "#df_spark=spark.read.parquet(\"sas_data1\")\n",
    "\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "#Drop duplicates and NaN values from the dataframe\n",
    "df_spark.dropna()\n",
    "df_spark.dropDuplicates()\n",
    "df_spark.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "--> The immigration data will be aggregated by city/year\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "1. Extract the data files, cleanse it and write it as parquet files into S3 buckets for each of the tables - dimensions/facts\n",
    "2. Load the data from S3 buckets into AWS Redshift database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      "\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+----+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype| age|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+----+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|37.0|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|25.0|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|55.0|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|28.0|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2| 4.0|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+----+-------------------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype| age|        arrdatetime|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+----+-------------------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|37.0|1970-01-01 05:42:53|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|25.0|1970-01-01 05:42:31|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|55.0|1970-01-01 05:42:25|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|28.0|1970-01-01 05:42:25|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2| 4.0|1970-01-01 05:42:25|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- arrdatetime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write code here\n",
    "\n",
    "# create age column from original biryear column for Immigration dataset\n",
    "df_spark = df_spark.withColumn(\"age\", (df_spark['i94yr'] - df_spark['biryear']))\n",
    "df_spark.printSchema()\n",
    "df_spark.show(5)\n",
    "\n",
    "#Convert arrdate into date datatype in immigration file\n",
    "\n",
    "# create datetime columns for arrival and departure times \n",
    "get_datetime = udf(lambda x: datetime.fromtimestamp(x), T.TimestampType())\n",
    "df_spark = df_spark.withColumn(\"arrdatetime\",get_datetime(df_spark['arrdate']))\n",
    "#df_spark = df_spark.withColumn(\"depdatetime\",get_datetime(df_spark['depdate']))\n",
    "df_spark.show(5)\n",
    "df_spark.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+-------+-------+-------------------+-------+-------+-------+------+-------+--------------+-----+--------+----+\n",
      "|cicid| i94yr|i94mon|i94port|i94addr|        arrdatetime|depdate|i94visa|biryear|gender|airline|        admnum|fltno|visatype| age|\n",
      "+-----+------+------+-------+-------+-------------------+-------+-------+-------+------+-------+--------------+-----+--------+----+\n",
      "|  6.0|2016.0|   4.0|    XXX|   null|1970-01-01 05:42:53|   null|    2.0| 1979.0|  null|   null| 1.897628485E9| null|      B2|37.0|\n",
      "|  7.0|2016.0|   4.0|    ATL|     AL|1970-01-01 05:42:31|   null|    3.0| 1991.0|     M|   null|  3.73679633E9|00296|      F1|25.0|\n",
      "| 15.0|2016.0|   4.0|    WAS|     MI|1970-01-01 05:42:25|20691.0|    2.0| 1961.0|     M|     OS|  6.66643185E8|   93|      B2|55.0|\n",
      "| 16.0|2016.0|   4.0|    NYC|     MA|1970-01-01 05:42:25|20567.0|    2.0| 1988.0|  null|     AA|9.246846133E10|00199|      B2|28.0|\n",
      "| 17.0|2016.0|   4.0|    NYC|     MA|1970-01-01 05:42:25|20567.0|    2.0| 2012.0|  null|     AA|9.246846313E10|00199|      B2| 4.0|\n",
      "| 18.0|2016.0|   4.0|    NYC|     MI|1970-01-01 05:42:25|20555.0|    1.0| 1959.0|  null|     AZ|9.247103803E10|00602|      B1|57.0|\n",
      "| 19.0|2016.0|   4.0|    NYC|     NJ|1970-01-01 05:42:25|20558.0|    2.0| 1953.0|  null|     AZ|9.247139923E10|00602|      B2|63.0|\n",
      "| 20.0|2016.0|   4.0|    NYC|     NJ|1970-01-01 05:42:25|20558.0|    2.0| 1959.0|  null|     AZ|9.247161383E10|00602|      B2|57.0|\n",
      "| 21.0|2016.0|   4.0|    NYC|     NY|1970-01-01 05:42:25|20553.0|    2.0| 1970.0|  null|     AZ|9.247079603E10|00602|      B2|46.0|\n",
      "| 22.0|2016.0|   4.0|    NYC|     NY|1970-01-01 05:42:25|20562.0|    1.0| 1968.0|  null|     AZ|9.247848973E10|00608|      B1|48.0|\n",
      "| 23.0|2016.0|   4.0|    NYC|     NY|1970-01-01 05:42:25|20671.0|    2.0| 1964.0|  null|     TK|9.250139443E10|00001|      B2|52.0|\n",
      "| 24.0|2016.0|   4.0|    TOR|     MO|1970-01-01 05:42:25|20554.0|    2.0| 1983.0|  null|     MQ|9.249090503E10|03348|      B2|33.0|\n",
      "| 27.0|2016.0|   4.0|    BOS|     MA|1970-01-01 05:42:25|20549.0|    1.0| 1958.0|     M|     LH|9.247876383E10|00422|      B1|58.0|\n",
      "| 28.0|2016.0|   4.0|    ATL|     MA|1970-01-01 05:42:25|20549.0|    1.0| 1960.0|     F|     LH|9.247890033E10|00422|      B1|56.0|\n",
      "| 29.0|2016.0|   4.0|    ATL|     MA|1970-01-01 05:42:25|20561.0|    2.0| 1954.0|     M|     AZ|9.250378143E10|00614|      B2|62.0|\n",
      "| 30.0|2016.0|   4.0|    ATL|     NJ|1970-01-01 05:42:25|20578.0|    2.0| 1967.0|     M|     OS|9.247020943E10|00089|      B2|49.0|\n",
      "| 31.0|2016.0|   4.0|    ATL|     NY|1970-01-01 05:42:25|20611.0|    2.0| 1973.0|     M|     OS|9.247128923E10|00089|      B2|43.0|\n",
      "| 33.0|2016.0|   4.0|    HOU|     TX|1970-01-01 05:42:25|20554.0|    2.0| 1963.0|     F|     TK|9.250930163E10|00033|      B2|53.0|\n",
      "| 34.0|2016.0|   4.0|    NYC|     CT|1970-01-01 05:42:25|   null|    2.0| 1968.0|     M|     AZ|9.247042023E10|00602|      B2|48.0|\n",
      "| 35.0|2016.0|   4.0|    NYC|     CT|1970-01-01 05:42:25|   null|    2.0| 1942.0|     F|     TK|  6.69712185E8|    1|      B2|74.0|\n",
      "+-----+------+------+-------+-------+-------------------+-------+-------+-------+------+-------+--------------+-----+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Read immigration data file and extract columns to create dim_visitors table\n",
    "visitors_table = df_spark['cicid', 'i94yr', 'i94mon', 'i94port', 'i94addr','arrdatetime', 'depdate', 'i94visa', 'biryear','gender', 'airline', 'admnum', 'fltno', 'visatype', 'age']\n",
    "#print(visitors_table)\n",
    "#visitors_table.show(5)\n",
    "visitors_table.createOrReplaceTempView(\"dim_visitors\")\n",
    "\n",
    "spark.sql(\"SELECT * from dim_visitors\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n",
      "DataFrame[_c0: string, _c1: string, _c9: string, _c2: string, _c3: string, _c4: string, _c5: string, _c7: string]\n",
      "+----------------+-------------+---+----+------+------+------+-----+\n",
      "|             _c0|          _c1|_c9| _c2|   _c3|   _c4|   _c5|  _c7|\n",
      "+----------------+-------------+---+----+------+------+------+-----+\n",
      "|   Silver Spring|     Maryland| MD|33.8| 40601| 41862| 82463|30908|\n",
      "|          Quincy|Massachusetts| MA|41.0| 44129| 49500| 93629|32935|\n",
      "|          Hoover|      Alabama| AL|38.5| 38040| 46799| 84839| 8229|\n",
      "|Rancho Cucamonga|   California| CA|34.5| 88127| 87105|175232|33878|\n",
      "|          Newark|   New Jersey| NJ|34.6|138040|143873|281913|86253|\n",
      "+----------------+-------------+---+----+------+------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Read city demographics data file and extract columns to create dim_city_demog table\n",
    "\n",
    "df_city = spark.read.option(\"delimiter\", \";\").csv('/home/workspace/us-cities-demographics.csv')\n",
    "df_city.printSchema()\n",
    "\n",
    "#Remove first row/header\n",
    "#df_city = df_city.filter(df['_c0'] == \"City\")\n",
    "#df_city.show(5)\n",
    "\n",
    "#city_table = df_city['city', 'state', 'state code', 'median age', 'Male Population', 'Female Population', 'Total Population', 'Foreign-born']\n",
    "city_table = df_city['_c0', '_c1', '_c9', '_c2', '_c3', '_c4', '_c5', '_c7']\n",
    "print(city_table)\n",
    "\n",
    "#Remove first row/header\n",
    "city_table = city_table.filter(city_table['_c0'] != 'City')\n",
    "\n",
    "city_table.show(5)\n",
    "df_city.createOrReplaceTempView(\"dim_city_demog\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  _c0|          _c1|                 _c2|         _c3|      _c4|        _c5|       _c6|         _c7|     _c8|      _c9|      _c10|                _c11|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string, _c11: string]\n",
      "+----+-------------+--------------------+----+---+---+-----+------------+----+----+----+--------------------+\n",
      "| _c0|          _c1|                 _c2| _c3|_c4|_c5|  _c6|         _c7| _c8| _c9|_c10|                _c11|\n",
      "+----+-------------+--------------------+----+---+---+-----+------------+----+----+----+--------------------+\n",
      "| 00A|     heliport|   Total Rf Heliport|  11| NA| US|US-PA|    Bensalem| 00A|null| 00A|-74.9336013793945...|\n",
      "|00AA|small_airport|Aero B Ranch Airport|3435| NA| US|US-KS|       Leoti|00AA|null|00AA|-101.473911, 38.7...|\n",
      "|00AK|small_airport|        Lowell Field| 450| NA| US|US-AK|Anchor Point|00AK|null|00AK|-151.695999146, 5...|\n",
      "|00AL|small_airport|        Epps Airpark| 820| NA| US|US-AL|     Harvest|00AL|null|00AL|-86.7703018188476...|\n",
      "|00AR|       closed|Newport Hospital ...| 237| NA| US|US-AR|     Newport|null|null|null| -91.254898, 35.6087|\n",
      "+----+-------------+--------------------+----+---+---+-----+------------+----+----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Read airport codes data file and extract columns to create dim_airport table\n",
    "\n",
    "df_airport = spark.read.csv('/home/workspace/airport-codes_csv.csv')\n",
    "df_airport.printSchema()\n",
    "df_airport.show(5)\n",
    "\n",
    "#airport_table = df_airport['ident', 'type', 'name', 'iso_country', 'iso_region', 'municipality', 'local_code']\n",
    "airport_table = df_airport['_c0', '_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11']\n",
    "print(airport_table)\n",
    "\n",
    "#Remove first row/header\n",
    "airport_table = airport_table.filter(airport_table['_c0'] != 'ident')\n",
    "airport_table.head()\n",
    "\n",
    "airport_table.show(5)\n",
    "df_airport.createOrReplaceTempView(\"dim_airport\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-------+-------------------+--------+------+----+----------+-------+--------+----------+---------------+-----------------+----------------+------------+-------+-------------+--------------------+----+---+---+-----+--------------+----+----+----+--------------------+\n",
      "| i94yr|i94mon|i94port|i94addr|        arrdatetime|visatype|gender| age|      City|  State|State_CD|Median_Age|Male_Population|Female_Population|Total_Population|Foreign_Born|    _c0|          _c1|                 _c2| _c3|_c4|_c5|  _c6|           _c7| _c8| _c9|_c10|                _c11|\n",
      "+------+------+-------+-------+-------------------+--------+------+----+----------+-------+--------+----------+---------------+-----------------+----------------+------------+-------+-------------+--------------------+----+---+---+-----+--------------+----+----+----+--------------------+\n",
      "|2016.0|   4.0|    ATL|     AL|1970-01-01 05:42:31|      F1|     M|25.0|Birmingham|Alabama|      AL|      35.6|         102122|           112789|          214911|        8258|US-0888|       closed|       Lyons Airpark|1220| NA| US|US-AL|    Rainsville|null|null|null|-85.793858, 34.48...|\n",
      "|2016.0|   4.0|    ATL|     AL|1970-01-01 05:42:31|      F1|     M|25.0|Birmingham|Alabama|      AL|      35.6|         102122|           112789|          214911|        8258|US-0686|small_airport|       Treeo Airport| 130| NA| US|US-AL|        Daphne|4AL3|null|4AL3|-87.822583, 30.59...|\n",
      "|2016.0|   4.0|    ATL|     AL|1970-01-01 05:42:31|      F1|     M|25.0|Birmingham|Alabama|      AL|      35.6|         102122|           112789|          214911|        8258|US-0685|small_airport|Rocky Springs Air...| 649| NA| US|US-AL|        Bremen|AL30|null|AL30|-87.077074, 33.98...|\n",
      "|2016.0|   4.0|    ATL|     AL|1970-01-01 05:42:31|      F1|     M|25.0|Birmingham|Alabama|      AL|      35.6|         102122|           112789|          214911|        8258|US-0684|     heliport|Plant Franklin He...| 523| NA| US|US-AL|Smiths Station|2AL0|null|2AL0|-85.094584, 32.60...|\n",
      "|2016.0|   4.0|    ATL|     AL|1970-01-01 05:42:31|      F1|     M|25.0|Birmingham|Alabama|      AL|      35.6|         102122|           112789|          214911|        8258|US-0683|     heliport|Montgomery Crew H...| 186| NA| US|US-AL|    Montgomery|1AL3|null|1AL3|-86.256073, 32.40...|\n",
      "+------+------+-------+-------+-------------------+--------+------+----+----------+-------+--------+----------+---------------+-----------------+----------------+------------+-------+-------------+--------------------+----+---+---+-----+--------------+----+----+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join the tables to create FACT table\n",
    "\n",
    "fact_immigrations_df = spark.sql(\"\"\"\n",
    "SELECT a.i94yr, a.i94mon, a.i94port, a.i94addr, a.arrdatetime, a.visatype, a.gender, a.age, \n",
    "b._c0 as City, b._c1 as State, b._c9 as State_CD, b._c2 as Median_Age, b._c3 as Male_Population, \n",
    "b._c4 as Female_Population, b._c5 as Total_Population, b._c7 as Foreign_Born, c._c0 as Airport_Code,\n",
    "c._c2 as Airport_Name, c._c6 as Airport_Region\n",
    "FROM dim_visitors a\n",
    "JOIN dim_city_demog b ON (a.i94addr = b._c9)\n",
    "JOIN dim_airport c ON (a.i94addr = substr(c._c6,4,2))\n",
    "\"\"\")\n",
    "    \n",
    "fact_immigrations_df.limit(5).show()\n",
    "#fact_immigrations_df.limit(5)\n",
    "#print(fact_immigrations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-------+-------------------+--------+------+----+----------+-------+--------+----------+---------------+-----------------+----------------+------------+-------+-------------+--------------------+----+---+---+-----+--------------+----+----+----+--------------------+\n",
      "| i94yr|i94mon|i94port|i94addr|        arrdatetime|visatype|gender| age|      City|  State|State_CD|Median_Age|Male_Population|Female_Population|Total_Population|Foreign_Born|    _c0|          _c1|                 _c2| _c3|_c4|_c5|  _c6|           _c7| _c8| _c9|_c10|                _c11|\n",
      "+------+------+-------+-------+-------------------+--------+------+----+----------+-------+--------+----------+---------------+-----------------+----------------+------------+-------+-------------+--------------------+----+---+---+-----+--------------+----+----+----+--------------------+\n",
      "|2016.0|   4.0|    ATL|     AL|1970-01-01 05:42:31|      F1|     M|25.0|Birmingham|Alabama|      AL|      35.6|         102122|           112789|          214911|        8258|US-0888|       closed|       Lyons Airpark|1220| NA| US|US-AL|    Rainsville|null|null|null|-85.793858, 34.48...|\n",
      "|2016.0|   4.0|    ATL|     AL|1970-01-01 05:42:31|      F1|     M|25.0|Birmingham|Alabama|      AL|      35.6|         102122|           112789|          214911|        8258|US-0686|small_airport|       Treeo Airport| 130| NA| US|US-AL|        Daphne|4AL3|null|4AL3|-87.822583, 30.59...|\n",
      "|2016.0|   4.0|    ATL|     AL|1970-01-01 05:42:31|      F1|     M|25.0|Birmingham|Alabama|      AL|      35.6|         102122|           112789|          214911|        8258|US-0685|small_airport|Rocky Springs Air...| 649| NA| US|US-AL|        Bremen|AL30|null|AL30|-87.077074, 33.98...|\n",
      "|2016.0|   4.0|    ATL|     AL|1970-01-01 05:42:31|      F1|     M|25.0|Birmingham|Alabama|      AL|      35.6|         102122|           112789|          214911|        8258|US-0684|     heliport|Plant Franklin He...| 523| NA| US|US-AL|Smiths Station|2AL0|null|2AL0|-85.094584, 32.60...|\n",
      "|2016.0|   4.0|    ATL|     AL|1970-01-01 05:42:31|      F1|     M|25.0|Birmingham|Alabama|      AL|      35.6|         102122|           112789|          214911|        8258|US-0683|     heliport|Montgomery Crew H...| 186| NA| US|US-AL|    Montgomery|1AL3|null|1AL3|-86.256073, 32.40...|\n",
      "+------+------+-------+-------+-------------------+--------+------+----+----------+-------+--------+----------+---------------+-----------------+----------------+------------+-------+-------------+--------------------+----+---+---+-----+--------------+----+----+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigrations_df.createOrReplaceTempView(\"fact_immigration\")\n",
    "\n",
    "fact_immigrations_df.dropDuplicates()\n",
    "fact_immigrations_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write visitors table to parquet files partitioned by year and city\n",
    "visitors_table.write.mode(\"overwrite\").partitionBy(\"i94yr\").parquet('dim_visitors.pq')\n",
    "#visitors_table.write.mode(\"overwrite\").partitionBy(\"i94yr\").parquet('s3://udacity-capstone-proj/dim_visitors/')\n",
    "\n",
    "# write airport table to parquet files \n",
    "airport_table.write.mode(\"overwrite\").parquet('dim_airport.pq')\n",
    "\n",
    "# write city table to parquet files \n",
    "city_table.write.mode(\"overwrite\").parquet('dim_city_demog.pq')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write FACT_IMMIGRATION table to parquet files partitioned by year and city\n",
    "fact_immigrations_df.limit(100).write.mode(\"overwrite\").partitionBy(\"i94yr\",\"i94addr\").parquet('fact_immig.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define S3 Buckets and create Data Lake\n",
    "\n",
    "#s3 = boto3.resource('s3')\n",
    "s3 = boto3.client('s3')\n",
    "BUCKET = 'udacity-capstone-proj'\n",
    "PREFIX1 = 'dim_visitors'\n",
    "PREFIX2 = 'dim_airport'\n",
    "PREFIX3 = 'dim_city_demog'\n",
    "PREFIX4 = 'fact_immigrations'\n",
    "root_path1 = '/home/workspace/dim_visitors.pq' # local folder \n",
    "root_path2 = '/home/workspace/dim_airport.pq' # local folder \n",
    "root_path3 = '/home/workspace/dim_city_demog.pq' # local folder \n",
    "root_path4 = '/home/workspace/fact_immig.pq' # local folder \n",
    "\n",
    "#s3.meta.client.upload_file('/home/workspace/dim_visitors.pq', BUCKET, PREFIX)\n",
    "\n",
    "#Upload Parquet files to S3 Bucket\n",
    "for path, subdirs, files in os.walk(root_path1):\n",
    "            path = path.replace(\"\\\\\",\"/\")\n",
    "            directory_name = path.replace(root_path1,\"\")\n",
    "            for file in files:\n",
    "                s3.upload_file(os.path.join(path, file),BUCKET, PREFIX1)\n",
    "\n",
    "#Upload Parquet files to S3 Bucket\n",
    "for path, subdirs, files in os.walk(root_path2):\n",
    "            path = path.replace(\"\\\\\",\"/\")\n",
    "            directory_name = path.replace(root_path2,\"\")\n",
    "            for file in files:\n",
    "                s3.upload_file(os.path.join(path, file),BUCKET, PREFIX2)\n",
    "                \n",
    "#Upload Parquet files to S3 Bucket\n",
    "for path, subdirs, files in os.walk(root_path3):\n",
    "            path = path.replace(\"\\\\\",\"/\")\n",
    "            directory_name = path.replace(root_path3,\"\")\n",
    "            for file in files:\n",
    "                s3.upload_file(os.path.join(path, file),BUCKET, PREFIX3)\n",
    "                \n",
    "#Upload Parquet files to S3 Bucket\n",
    "for path, subdirs, files in os.walk(root_path4):\n",
    "            path = path.replace(\"\\\\\",\"/\")\n",
    "            directory_name = path.replace(root_path4,\"\")\n",
    "            for file in files:\n",
    "                s3.upload_file(os.path.join(path, file),BUCKET, PREFIX4)                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3096313\n",
      "2892\n",
      "55076\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "\n",
    "#Check the record count in each table and ensure there is data\n",
    "cnt1 = spark.sql(\"SELECT * from dim_visitors\").count()\n",
    "print(cnt1)\n",
    "\n",
    "if cnt1 <= 0:\n",
    "    print(\"Error - No Data in dim_visitors !!\")\n",
    "    \n",
    "#Check the record count in each table and ensure there is data\n",
    "cnt1 = spark.sql(\"SELECT * from dim_city_demog\").count()\n",
    "print(cnt1)\n",
    "\n",
    "if cnt1 <= 0:\n",
    "    print(\"Error - No Data in dim_city_demog !!\")\n",
    "    \n",
    "#Check the record count in each table and ensure there is data\n",
    "cnt1 = spark.sql(\"SELECT * from dim_airport\").count()\n",
    "print(cnt1)\n",
    "\n",
    "if cnt1 <= 0:\n",
    "    print(\"Error - No Data in dim_airport !!\")\n",
    "    \n",
    "#Check the record count in each table and ensure there is data\n",
    "cnt1 = spark.sql(\"SELECT * from fact_immigration\").count()\n",
    "print(cnt1)\n",
    "\n",
    "if cnt1 <= 0:\n",
    "    print(\"Error - No Data in fact_immigrations !!\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tools and Technologies Used :\n",
    "    Python - The python wrapper script uses pandas dataframes to easily manipulate huge datasets.\n",
    "    \n",
    "    AWS S3 Storage - Data Lake is created on AWS cloud and the data is saved in columnar format in the form of parquet files saved in S3 which is \n",
    "    easily available and accessible on the cloud. (Allows extensibility to load the data from S3 to any database , ex: Redshift)\n",
    "    \n",
    "    Spark - The Spark instance used in the program enables parallelization of the code in a distributed cluster enabling scalability. PySpark SQL is used to \n",
    "    manipulate and load the data to perform ETL.\n",
    "    \n",
    "    Data Model - Star schema (Denormalized to support OLAP queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Frequency : The data need to be updated every day to record the number of i94 entries each day to track the number of immigrants, visitors etc. entering the US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If the data volume is increased by 100x, the ETL would need to be run in parallel mode on multiple clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If the data populates a dashboard that gets updated daily, create an Airflow DAG to be scheduled to run daily at 7am, which will pick the latest(previous days\n",
    "data) and processes the datafile and runs the ETL.                                                                                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If the database needs to be accessed by 100+ people, load the data from S3 into Redshift on AWS cloud which is scalable enough to support multiple concurrent \n",
    "user access on the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
